# Unsupervised Feature Learning via Non-Parametric Instance Discrimination

[paper link here](https://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0801.pdf)

## Abstract

Can we learn a good feature representation that captures apparent similarity `among instances`, insted of classes,
by merely asking the `feature to be discriminative of individual instances?`

* They formulate thus intuition as a `non-parametric classification problem` at the instance-level, and use `NCE`
to tackle the computational challenges imposed by the large number of classification
* surpassess SOTA on ImageNet classificatino by a large margin.

## Introduction

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/Representation_1_1.png" width=500>

> Fig 1 shows that an image from class leopard is rated much higher by class jaguar rather than by class bookcase.

Such observations reveal that a typical discriminative learning method can `automatically discover apparent
similarity among semantic categories`, `without being explicitly guided to do so.`

**Can we learn a meaningful metric that reflects apparent similarity among instances via pure discriminative learning?**

However, they also face a major challenge, now that `the number of 'classes' is the size of the entire training set.`
They tackle this challenge by `approximating the full softmax distribution` with [NCE](https://www.kdnuggets.com/2019/07/introduction-noise-contrastive-estimation.html), and by resorting to
a `proximal regularization method to stabilize the learning process.`

They advocate a non-parametric approach for bot

## Approach

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/Representation_1_2.png" width=1000>

Our goal is to learn an embedding function *v = f<sub>θ</sub>(x)* without supervision. f<sub>θ</sub> is a deep neural network
with parameters θ, mapping image *x* to feature *v*.

This Embeding would induces a metric over the image space.

> d(x, y) = ||f<sub>θ</sub>(x) - f<sub>θ</sub>(y)||

A good embedding should map `visually similar images closer to each other.`

### 3.1 Non-Parametric Softmax Classifier

Under the conventional parametric softmax formulation, the probabilty of it being recognized as i-th example is

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/Representation_1_3.png" width=500>

The problem with the parametric softmax classifier in **Eq. (1)** is that the weight vector *W* serves as a class
prototype, `preventing explicitly comparsions between instances.`

Therefore, they proposed a non-parametric variant of **Eq. (1)**. (replacing *W* to *V*)

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/Representation_1_4.png" width=500>

* We enforce ||v|| = 1 via a L2-normalization layer.
* τ is a [temperature parameter](https://velog.io/@jkl133/temperature-parameter-in-learner-fastai) that `controls the concentration level of the distribution`

> When get rid of *W* weighted vectors, our learning objective `focuses entirely on the feature representation.` Also, computationally,
> non-parametric formulation `eliminates the need for computing and storing the gradients` for *W*, making it `more scalable for big data applications.`
 
The learning objective is to minimize the negative log-likelihood over the traing set, as

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/Representation_1_5.png" width=500>

**Learning with a memory bank** : 

### 3.2 Noise Contrastive Estimation

We adapt NCE to our problem, `in order to tackle the difficulty of computing the similarity to all the instances` in the training set.

NCE reduces the computational complexity from O(n) to O(1) per sample.

### 3.3 Proximal Regularization

Unlike typical classification settings where each class has many instances, we only have `one instance per class.`
Therefore, the learning process `oscillates a lot` from random sampling fluctuation.
That's why they employ the proximal optimization method.

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/Representation_1_6.png" width=500>

> Fig. 3 shows that, empirically, proximal regularization helps stabilize training, speed up convergence, and improve the
> representation.
