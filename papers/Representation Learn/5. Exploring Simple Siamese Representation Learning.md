# Exploring Simple Siamese Representation Learning

[paper link here](https://arxiv.org/pdf/2011.10566.pdf)

## Abstract

In this papaer, We report that `simple Siamese networks can learn meaningful representations` even using none of the following.
* negative sample pairs
* large batches
* momentum encoders

Our experiments show that collapsing solutions do exist for the loss and structure, but a `stop gradient operation plays an essential
role` in preventing collapsing.

## Introduction

An undesird trivial solution to Siamese networks is all outputs 'collapsing' to a constant. There have been several
general strategies for preventing Siamese networks form collapsing.
* In `SimCLR`, repulses different images while attracting the same image's two views.
* Clustering is another way of avoiding constant output (SWAV)
* `BYOL` relies only on positive pairs but it does not collapse in case a momentum encoder is used.

In this paper, we report that simple Siamese networks can work surprisingly well with none of the above
strategies for preventing collapsing.

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/Representation_5_1.png" width=500>

We empirically show that collapsing solutions do exist, but a `stop gradient operation is critical` to prevent such
solutions.

Our simple baseline suggests that the Siamese architectures can be an essential reason for the common success of the related methods.
* Siamese networks can naturally introduce [inductive biases](https://velog.io/@euisuk-chung/Inductive-Biasëž€) for modeling invariance, as by definition 'invariance' means that two observatinos of the same concept should produce the same outputs.

## Method

Our architecture takes input two randomly augmented views *x1* and *x2* from an image *x*.

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/Representation_5_2.png" width=500>

The Two views are processed by and encoder network *f* consisting of a backbone(ResNet) and a projection MLP head.
Denoting the two output vectors as *p<sub>1</sub> = h(f(x<sub>1</sub>))* and *z<sub>2</sub> = f(x<sub>2</sub>)*,
We minimize thier negative cosine similarity.

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/Representation_5_3.png" width=500>

Following, we define a symmetrized loss as

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/Representation_5_4.png" width=500>

This is defined for each image, and the total loss is averaged over all images. Its minimum possible value is -1.

An important component for out method to work is a `stop-gradient(stopgrad) operation.`

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/Representation_5_5.png" width=500>

This means that *z2* is `treated as a constant` in this term.

There are few **Baseline settings** below.

* **Optimizer** : We use SGD for pre-training. (Does not require large-batch optimizer such as LARS).
* **Projection MLP** : The projection MLP has BN applied to each fully-connected layer, including its output fc.
* Using ResNet-50 as the default backbone.
* **Experimental setup** : Do unsupervised pre-training on the 1000-class ImageNet training set without using labels.

