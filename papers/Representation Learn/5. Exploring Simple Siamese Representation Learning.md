# Exploring Simple Siamese Representation Learning

[paper link here](https://arxiv.org/pdf/2011.10566.pdf)

## Abstract

In this papaer, We report that siamese `simple Siamese networks can learn meaningful representations` even using none of the following.
* negative sample pairs
* large batches
* momentum encoders

Our experiments show that collapsing solutions do exist for the loss and structure, but a `stop gradient operation plays an essential
role` in preventing collapsing.

## Introduction

An undesird trivial solution to Siamese networks is all outputs 'collapsing' to a constant. There have been several
general strategies for preventing Siamese networks form collapsing.
* In `SimCLR`, repulses different images while attracting the same image's two views.
* Clustering is another way of avoiding constant output (SWAV)
* `BYOL` relies only on positive pairs but it does not collapse in case a momentum encoder is used.

In this paper, we report that simple Siamese networks can work surprisingly well with none of the above
strategies for preventing collapsing.

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/Representation_5_1.png" width=500>

We empirically show that collapsing solutions do exist, but a `stop gradient operation is critical` to prevent such
solutions.

Our simple baseline suggests that the Siamese architectures can be an essential reason for the common success of the related methods.
* Siamese networks can naturally introduce [inductive biases](https://velog.io/@euisuk-chung/Inductive-Biasëž€) for modeling invariance, as by definition 'invariance' means that two observatinos of the same concept should produce the same outputs.

## Method







