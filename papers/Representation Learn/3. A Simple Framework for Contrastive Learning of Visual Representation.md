# A Simple Framework for Constrastive Learning of Visual Representation

[paper link here.](https://arxiv.org/pdf/2002.05709.pdf)

## Abstract

This paper proprosed contrastive self-supervised learning algorithms `without requiring specialized architectures or a memory bank.`

Especially, there are 3 points.

* Composition of `data argumentations` plays a critical role in defining effective augmentations plays a critical role in
defining effective predictive tasks.
* `Learnable non-linear transformation` between the representation and the contrastive loss substantially improves the quality
of the learned representations.
* Contrastive learning benefits from `larger batch sizes` and `more training steps` compared to supervising learning.

## Introduction

This work introduce a simple framework for contrastive learning of visual representations, which is called `SimCLR`

There are 4 major components that are systmatically studied.

* Composition of multiple data augmentation operations is crucial in defining the contrastive predictions tasks.
* learnable nonlinear transformation between representation and the contrastive loss substantially improves the quality
of the learned representations.
* Representation learning with contrastive cross entropy loss benefits from normalized embeddings and an appropriately
adjusted temperature parameter.
* Contrastive learning benefits from larger batch sizes and longer training compared to its supervised counterpart.

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/Representation_3_1.png" width=500>

