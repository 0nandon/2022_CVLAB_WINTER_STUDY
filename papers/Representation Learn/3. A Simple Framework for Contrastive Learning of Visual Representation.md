# A Simple Framework for Constrastive Learning of Visual Representation

[paper link here.](https://arxiv.org/pdf/2002.05709.pdf)

## Abstract

This paper proprosed contrastive self-supervised learning algorithms `without requiring specialized architectures or a memory bank.`

Especially, there are 3 points.

* Composition of `data argumentations` plays a critical role in defining effective augmentations plays a critical role in
defining effective predictive tasks.
* `Learnable non-linear transformation` between the representation and the contrastive loss substantially improves the quality
of the learned representations.
* Contrastive learning benefits from `larger batch sizes` and `more training steps` compared to supervising learning.

## Introduction

This work introduce a simple framework for contrastive learning of visual representations, which is called `SimCLR`

There are 4 major components that are systmatically studied.

* `Composition of multiple data augmentation` operations is crucial in defining the contrastive predictions tasks.
* `learnable nonlinear transformation` between representation and the contrastive loss substantially improves the quality
of the learned representations.
* `Representation learning with contrastive cross entropy loss` benefits from normalized embeddings and an appropriately
adjusted temperature parameter.
* Contrastive learning benefits from `larger batch sizes` and `longer training` compared to its supervised counterpart.


## Method

### 2.1 The contrastive Learning Framework
<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/Representation_3_1.png" width=500>

SimCLR learns representations by maximizing agreement between differently augmented views of the same data example
via a contrastive loss in the latent space.

* A `stohastic data augmentation module` that transforms any given data example randomly resulting in tow correlated views
of the same example.
* A neural network base encoder *f()* that extracts representation vectors from augmented data examples.
* *g()* is non linear projection head. (MLP with one hidden layer and ReLU non-linearity.)
* A *contrastive loss function* defined for a contrastive predictions task aims.

### 2.2 Training with Large Batch Size

* Training with large batch size may be unstable when using standard SGD/Momentum with linear learning rate scaling.
They use the [LARS](https://www.kakaobrain.com/blog/113) optimizer for all batch sizes.
* `Global BN` : BN mean and variance are typically aggregated locally per device. In our contrastive learning, as positive
pairs are computed in the same device, the model can exploit the local information leakage to improve prediction accuracy
without improving representations. We address this issue by `aggregating BN mean and variance over all devices during training.`
<br>
[More info about Global BN in SimCLR](https://www.youtube.com/watch?v=4wddWrTlLsw)

### 2.3 Evaluation Protocol

* **Daetaset and Metrics** : To evaluate the learned representations, we follow the widely used `linear evaluation protocol.`
* **Default setting** : Using random crop and reseize, color distortions, and Gaussian blur for data augmentations.



## Data Augmentation for Contrastive Representation Learning


