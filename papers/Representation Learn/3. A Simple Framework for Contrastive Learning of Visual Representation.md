# A Simple Framework for Constrastive Learning of Visual Representation

[paper link here.](https://arxiv.org/pdf/2002.05709.pdf)

## Abstract

This paper proprosed contrastive self-supervised learning algorithms `without requiring specialized architectures or a memory bank.`

Especially, there are 3 points.

* Composition of `data argumentations` plays a critical role in defining effective augmentations plays a critical role in
defining effective predictive tasks.
* `Learnable non-linear transformation` between the representation and the contrastive loss substantially improves the quality
of the learned representations.
* Contrastive learning benefits from `larger batch sizes` and `more training steps` compared to supervising learning.

## Introduction

This work introduce a simple framework for contrastive learning of visual representations, which is called `SimCLR`

There are 4 major components that are systmatically studied.

* `Composition of multiple data augmentation` operations is crucial in defining the contrastive predictions tasks.
* `learnable nonlinear transformation` between representation and the contrastive loss substantially improves the quality
of the learned representations.
* `Representation learning with contrastive cross entropy loss` benefits from normalized embeddings and an appropriately
adjusted temperature parameter.
* Contrastive learning benefits from `larger batch sizes` and `longer training` compared to its supervised counterpart.

They randomly sample a minibatch of N examples and define the contrastive prediction task on pairs of augmented examples
derived from the minibatch, resulting 2N data points. They `do not sample negative examples explicitly.` Instead, given
a positive pair, they treat the other 2(N-1) augmented examples within a minibatch as negative examples.

Pic below is the contrastive function where *sim(u, v) = u<sup>T</sup>v / ||u||.||v||.*

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/Representation_3_2.png" width=500>

They called this, `NT-Xent loss.`

## Method

### 2.1 The contrastive Learning Framework
<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/Representation_3_1.png" width=500>

SimCLR learns representations by maximizing agreement between differently augmented views of the same data example
via a contrastive loss in the latent space.

* A `stohastic data augmentation module` that transforms any given data example randomly resulting in tow correlated views
of the same example.
* A neural network base encoder *f()* that extracts representation vectors from augmented data examples.
* *g()* is non linear projection head. (MLP with one hidden layer and ReLU non-linearity.)
* A *contrastive loss function* defined for a contrastive predictions task aims.

### 2.2 Training with Large Batch Size

* Training with large batch size may be unstable when using standard SGD/Momentum with linear learning rate scaling.
They use the [LARS](https://www.kakaobrain.com/blog/113) optimizer for all batch sizes.
* `Global BN` : BN mean and variance are typically aggregated locally per device. In our contrastive learning, as positive
pairs are computed in the same device, the model can exploit the local information leakage to improve prediction accuracy
without improving representations. We address this issue by `aggregating BN mean and variance over all devices during training.`
[> More info about Global BN in SimCLR](https://www.youtube.com/watch?v=4wddWrTlLsw)

### 2.3 Evaluation Protocol

* **Daetaset and Metrics** : To evaluate the learned representations, we follow the widely used `linear evaluation protocol.`
* **Default setting** : Using random crop and reseize, color distortions, and Gaussian blur for data augmentations.
Also, using `ResNet-50 as the base encoder network`, and `2-layer MLP projection head` to project the representation to a 128-dimensional space.

## Data Augmentation for Contrastive Representation Learning

Many exisiting approaches define contrastive prediction tasks by changing the architecture.
This paper show that this complexity can be avoided by performing simple random cropping of target images.

### 3.1 Composition of data augmentation

> There are 2 types of augmentations. One is spatial/geometric transformation of data, such as cropping and resizing, rotation,
> cutout. The other one is appearance transformation, such as color distortion, Gaussian blur, and sober filtering.

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/Representation_3_3.png" width=500>

Figure 5 shows linear evaluation results under individual and composition of transformations. We observe that `no
single transformation suffices to learn good representations`, even though the model can almost perfectly identify the
positive pairs in the contrastive task.

They also conjecture in this section, that one serious issue when using only random cropping as data augmentation
is that most patches from an image share a `similar color distortion`. Neural nets may exploit this shortcut to
solve the predictive task. Therefore, it is `critical to compose cropping with color distortion in order to learn
generalizable featuers.`

### 3.2 Contrastive learning needs stronger data augmentation than supervised learning.

They found in some experiments that `unsupervised contrastive learning benefits from stronger data augmentation than
supervised learning.` It shows that data augmentation that does not yield accuracy benefits for supervised learning `can still help
considerably with contrastive learning.`

## Architectures for Encoder and Head

### 4.1 Unsupervised contrastive laerning benefits from bigger models.

Unsurprisingly, They suggested that unsupervised-learning benefits more from bigger models than its supervised learning.

### 4.2 A nonlinear projection head improves the representation quality of the layer before it.

They conjecture that the importance of using the representation before the nonlinear projection is due to loss of information
induced by the contrastive loss. In particular, *z=g(h)* is trained to be invariant to data transformation. Thus, *g* can `remove
information that may be useful for the downstream task, such as the color or orientation of objects.`

By leveraging the non linear transformation *g(.)*, more information can be formed and maintained in *h*.

## Loss Functions and Batch Size

### 5.1 Normalized cross entropy loss with adjustable temperature works better than alternatives.

They compare NT-Xent loss against other commonly used contrastive loss functins, such as logistic loss,
and margin loss.

> Unlike cross entropy, other objective functions do not weigh the negatives by their relative hardness.
> As a result, one must apply semi-hard negative mining for these loss functions.

### 5.2 Contrastive learning benefits (more) from larger batch sizes and longer training.

In contrast to supervised learning, in contrastive learning, larger batch sizes provide `more negative examples`,
`facilitation convergence.`
