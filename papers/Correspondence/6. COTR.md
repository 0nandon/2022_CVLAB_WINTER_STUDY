
# COTR : Correspondence Transformer for Matching Across Images

[paper link here](https://arxiv.org/pdf/2103.14167.pdf)

## Abstract

We propose a novel framework for finding correspondences in images based on a deep nueral network that, given two images
and a query point in one of them, find its correspondence in the other.

Importantly, in order to capture both local and global priors, and to let our model relate between image regions using
the most relevant among said priors, we realize our network using a `transformer.`

## Introduction

Traditionally, two fundamental research directions exist for this probelm.
* One is to extract `sets of sparse keypoints` from both images and match them in order to minimize an alignment metric.
* The other is to interpret correspondence as a dense process, where `every pixel` in this first image maps to a pixel in the second image.

In this work, we present a solution that bridges this divide,
a novel network architecture that can express both forms of prior knowledge - `global and local` - and learn them implicitly from data.-

Differently from sparse methods, COTR can match arbitrary query points via this functional mapping, predicting only as
many matches as desired. Differently from dense methods, COTR learns `smoothness implicitly` and can deal with `large camera
motion effectively.`

Our work is the first to apply transformers to obtain accurate correspondences.

* we prepose a functional correspondence architecture that combines the strengths of dense and sparse methods.
* we show how to apply our method `recursively` at multiple scales during inference in order to compute highly accurate
correspondences.
* we demonstrate that COTR achieves SOTA performance in both dense and correspondence problems on multiple datasets and
tasks.
* we substantiate our design choices and show that the transformer is key to our approach by replacing it with a simple
model, based on MLP.

## Method

### 3.1 Problem formulation

Let *x* ∈ [0, 1]<sup>2</sup> be the normalized coordinates of the query point in image *I*, for which we wish to find
the corresponding point, *x*<sup>'</sup> ∈ [0, 1]<sup>2</sup>, in image *I*<sup>'</sup>.

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/COTR_1.png" width=500>

### 3.2 Network architecture

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/COTR_4.png" width=500>

We first `crop and resize` the input into 256 * 256 image, and convert it into a downsampled feature map size
16 * 16 * 256 with a shared CNN backbone. We then concatenate the representations for two corresponding images
*side by side*, forming a feature map size 16 * 32 * 256 to which we add positional encoding *P* of the coordinate
function to produce a *context feature map* **c**.

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/COTR_2.png" width=500>

We then feed the context feature map **c** to a transfomer encoder *T*<sub>e</sub>, and interpret its
results with a transformer decoder *T*<sub>D</sub>, along with the query point *x*, encoded by *P* - the positional
encoder.

We finally process the output of the transformer decoder with a fully connected layer *D* to obtain our estimate
for the corresponding point *x*<sup>'</sup>.

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/COTR_3.png" width=500>

#### Importance of context concatenation

Concatenation of the feature maps along the spatial dimension is critical, as it allows the transformer encoder *T*<sub>e</sub>
to `relate between locations within the image and across the images.`

* to allow the encoder to distinguish between pixels in the two images, we employ a single positional encoding for the
entire concatenated feature map.
* We concatenate along the spatial dimension rather than the channel dimension, as the latter would create artifical
relationships between `features coming form the same pixel locations in each image.`

> Concatenation allows the features in each map to be treated in a way that is similar to words in a sentence.

#### Linear positional encoding

We find it critical to use *linear* increase in frequency for the positional encoding, as opposed to the commonly used
log-linear strategy, which made our optimization unstable.

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/COTR_5.png" width=400>

#### Querying multiple points

We can simply input multiple queries at once, which the transformer decoder *T*<sub>D</sub> and the decoder *D* will translate
into multiple coordinates.

### 3.3 Inference

We next discuss how to apply our funcitional approach at inference time in order to abtain accurate correspondences.

#### Inference with recursive zoom-in

Applying the powerful transformer attention mechanism to vision probelms comes at a cost - it requires `heavily
downsampled feature maps`, which in our case naturally translates to poorly localized correspondences.

We address this by exploiting the functional nature of our approach, applying our network *F* recursively.

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/COTR_6.png" width=500>

We iteratively zoom into a previously estimated correspondence, on both images, in order to obtain a
refined estimate.

#### Compensating for scale differences

While matching images recursively, one must account for a `potential mismatch in scale` between images.

We achieve this by making the scale of the patch to crop proportional to the commonly visible
regions in each image, which we compute on the first step, using the whole images.

To extract this region, we compute the `cycle consistency error` at the coarsest level, for every pixel,
and threshold it at *T*<sub>visible</sub> = 5 pixels on the 256 * 256 image.

In subsequent stages - the zoom-ins - we simply adjust the crop sizes over *I* and *I*<sup>'</sup> so that
the relationship is proportional to the sum of valid pixels.

#### Dealing the images of arbitrary size

Our network expects images of fixed 256 * 256 shape. To process images of arbitrary size, in the initial step
we simply `resize` them to 256 * 256, and estimate the initail correspondences.

In subsequent zoom-ins, we crop square patches from the original image around estimated points, of a size
comensurate with the current zoom level, and resize them to 256 * 256.

#### Discarding erroneous correspondences

If we query a point is occluded or outside the viewport in other image, we simply rejected correspondences
that induce a cycle consistency eerror greater then *T*<sub>cycle</sub> = 5 pixels.

## Conclusion and future work

We introduced a functional network for image correspondence that is capable to address both sparse and dense matching
problems.










