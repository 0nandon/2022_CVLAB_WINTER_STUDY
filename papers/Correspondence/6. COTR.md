
# COTR : Correspondence Transformer for Matching Across Images

[paper link here](https://arxiv.org/pdf/2103.14167.pdf)

## Abstract

We propose a novel framework for finding correspondences in images based on a deep nueral network that, given two images
and a query point in one of them, find its correspondence in the other.

Importantly, in order to capture both local and global priors, and to let our model relate between image regions using
the most relevant among said priors, we realize our network using a `transformer.`

## Introduction

Traditionally, two fundamental research directions exist for this probelm.
* One is to extract `sets of sparse keypoints` from both images and match themm in order to minimize an alignment metric.
* The other is to interpret correspondence as a dense process, where `every pixel` in this first image maps to a pixel in the second image.

In this work, we present a solution that bridges this divide,
a novel network architecture that can express both forms of prior knowledge - `global and local` - and learn them implicitly from data.-

Differently from sparse methods, COTR can match arbitrary query points via this functional mapping, predicting only as
many matches as desired. Differently from dense methods, COTR learns `smoothness implicitly` and can deal with `large camera
motion effectively.`

Our work is the first to apply transformers to obtain accurate correspondences.

* we prepose a functional correspondence architecture that combines the strengths of dense and sparse methods.
* we show how to apply our method `recursively` at multiple scales during inference in order to compute highly accurate
correspondences.
* we demonstrate that COTR achieves SOTA performance in both dense and correspondence problems on multiple datasets and
tasks.
* we substantiate our design choices and show that the transformer is key to our approach by replacing it with a simple
model, based on MLP.

## Method

### 3.1 Problem formulation

Let *x* ∈ [0, 1]<sup>2</sup> be the normalized coordinates of the query point in image *I*, for which we wish to find
the corresponding point, *x*<sup>'</sup> ∈ [0, 1]<sup>2</sup>, in image *I*<sup>'</sup>.

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/COTR_1.png" width=600>





