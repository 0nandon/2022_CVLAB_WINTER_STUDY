
# Spatial Transformer Networks

[paper link here](https://arxiv.org/pdf/1506.02025.pdf)

[Tutorial code here](https://tutorials.pytorch.kr/intermediate/spatial_transformer_tutorial.html)

## Abstract

CNN define an exceptionally powerful class of models but are still limited by the lack of ability to be
`spatially invariant` to the input data in a computationally and parameter efficient manner.

In this work, we introduce a new learnable module, the `Spatial Transformer`, which `explicitly allows the
spatial manipulation of data` within the network.

This differentiable module can be inserted into existing convolutional architectures, giving neural
networks the ability to actively transform feature maps, conditional on the feature map itself,
`without any extra training supervision or modification to the optimisation process.`

## Introduction

A desirable property of a system which is able to reason about images is to `disentangle object pose` and `part deformation
from texture and shape.` The introduction of `local max-pooling layers` in CNNs has helped to satisfy this property.

However, due to the typically small spatial support for max-pooling this `spatial invariance is only realised over
a deep hierarchy of max-pooling and convolutions`, and the intermediate feature maps in a CNN `are not actually invariant
to large transformations of the input data.`

In this work, we introduce a `Spatial Transformer` module, that can be included into a standard neural network architecture
to provide spatial transformation capabilities.

Unlike pooling layers, `where the receptive fields are fixed and local`, the spatial transformer module is a dynamic mechanism
that `can actively spatially transform an image.`

**This can select not only regions of an image that are most relevant, but also to transform those regions to
a cannonical, expected pose to simplify recognition in the following layers.**

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/correspondence_2_1.png" width=700>

THis spatial transformer can be worked as attention, a key benefit of using attention is that transformed, lower resolution
inputs `can be used in favour of higher resolution raw inputs`, resulting in increased computational efficiency.

## Spatial Transformers

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/correspondence_2_2.png" width=700>

The spatial transformer mechanism is split into three parts.
* First, a localisation network takes the input feature map, and through a number of hidden layers outputs
the parameters of the spatial transformation that should be applied to the feature map.
* Then, the predicted transformation parameters are used to create a `sampling grid`, which is set of points where
the `input map should be sampled to produce the transformed output.`
* Finally, the feature map and the sampling grid are taken as inputs to the sampler, `producing the output map
sampled from the input at the grid points.`

### 3.1 Localisation Network

The localisation network takes the input feature map *U* ∈ *R*<sup>H * W * C</sup> with width *W*, height *H*, and *C* channels and
outputs θ, the parameters of the transformation *T*<sub>θ</sub> to be applied to the feature map.

The localisation network function `can take any form`, such as a fully-connected network or a convolutional network, but should
include a `final regression layer` to produce the transformation parameters θ.

### 3.2 Parameterised Sampling Grid

To perform a warping of the input feature map, each output pixel is computed by applying a sampling kernel centered at a
particular location in the input feature map.

In general, the output pixels are defined to lie on a regular gird *G* = { *G*<sub>*i*</sub> } of pixels
*G*<sub>*i*</sub> = (*x*<sub>i</sub><sup>t</sup>, *y*<sub>i</sub><sup>t</sup>), forming an output feature map *V*.

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/correspondence_2_3.png" width=700>

For clarity of exposition, assume for the moment that *T*<sub>θ</sub> is a 2D affine transformation A<sub>θ</sub>.
In the affine case, the pointwise transformation is

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/correspondence_2_4.png" width=700>

The transformation allows cropping, translation, rotation, scale, and skew to be applied to the input feature map.

Also, transformation can also be `more general`, such as a plane projective transformation with 8 parameters,
piece wise affine, or a thin plate spline.

### 3.3 Differentiable Image Sampling

To perform a spatial transformation of the input feature map, a sampler must take the set of sampling points
T<sub>θ</sub>(*G*), along with the input feature map *U* and produce the sampled output feature map *V*.

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/correspondence_2_5.png" width=700>

### 3.4 Spatial Transformer Networks

This is a self -contained module which can be dropped into a CNN architecture `at any point`, and in any number,
giving rise to *spatial transformer networks*.

This module is computationally `very fast and does not impair the training speed`, causing very little time overhead
when used naively, and even speedups in attentive models due to `subsequent downsampling` that can be applied to the
output of the transformer.

For some tasks, it may also be useful to feed the output of the localisation network, θ, forward to the rest of the network,
as it `explicitly encodes the transformation`, and hence `the pose, of a region or object.`

It is also possible to use spatial transformers to downsample or oversample a feature map.
However, with sampling kernels with a fixed, small spatial support, downsampling with a spaltial transformer can
cause `aliasing effects.`

Finally, it is possible to have multiple spatial transformers in a CNN. placing a multiple spatial transformers at increasing
depths of a network allow transformations of increasingly abstract representations.

One can also use multiple spatial transformers in `parallel` - this can be useful `if there are multiple objects.`
A limitation of this architecture in a purely feed-forward network is that the number of parallel spatial transformers
`limits the number of objects that the network can model.`

## Conclusion

In this paper, we introduced a new self-contained module for neural networks. This module can be dropped into a network
and perform explicit spatial transformations of features, providing an incredibly strong baseline that results SOTA performance.

Furthermore, the regressed transformation parameters from the spatial transformer are available as an output and could be used
for subsequent tasks.



