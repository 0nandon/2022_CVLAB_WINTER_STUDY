
# Spatial Transformer Networks

[paper link here](https://arxiv.org/pdf/1506.02025.pdf)

[Tutorial code here](https://tutorials.pytorch.kr/intermediate/spatial_transformer_tutorial.html)

## Abstract

CNN define an exceptionally powerful class of models but are still limited by the lack of ability to be
`spatially invariant` to the input data in a computationally and parameter efficient manner.

In this work, we introduce a new learnable module, the `Spatial Transformer`, which `explicitly allows the
spatial manipulation of data` within the network.

This differentiable module can be inserted into existing convolutional architectures, giving neural
networks the ability to actively transform feature maps, conditional on the feature map itself,
`without any extra training supervision or modification to the optimisation process.`

## Introduction

A desirable property of a system which is able to reason about images is to `disentangle object pose` and `part deformation
from texture and shape.` The introduction of `local max-pooling layers` in CNNs has helped to satisfy this property.

However, due to the typically small spatial support for max-pooling this `spatial invariance is only realised over
a deep hierarchy of max-pooling and convolutions`, and the intermediate feature maps in a CNN `are not actually invariant
to large transformations of the input data.`

In this work, we introduce a `Spatial Transformer` module, that can be included into a standard neural network architecture
to provide spatial transformation capabilities.

Unlike pooling layers, `where the receptive fields are fixed and local`, the spatial transformer module is a dynamic mechanism
that `can actively spatially transform an image.`

**This can select not only regions of an image that are most relevant, but also to transform those regions to
a cannonical, expected pose to simplify recognition in the following layers.**

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/correspondence_2_1.png" width=700>

THis spatial transformer can be worked as attention, a key benefit of using attention is that transformed, lower resolution
inputs `can be used in favour of higher resolution raw inputs`, resulting in increased computational efficiency.

## Spatial Transformers

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/correspondence_2_2.png" width=700>

The spatial transformer mechanism is split into three parts.
* First, a localisation network takes the input feature map, and through a number of hidden layers outputs
the parameters of the spatial transformation that should be applied to the feature map.
* Then, the predicted transformation parameters are used to create a `sampling grid`, which is set of points where
the `input map should be sampled to produce the transformed output.`
* Finally, the feature map and the sampling grid are taken as inputs to the sampler, `producing the output map
sampled from the input at the grid points.`

### 3.1 Localisation Network

The localisation network takes the input feature map *U* ∈ *R*<sup>H * W * C</sup> with width *W*, height *H*, and *C* channels and
outputs θ, the parameters of the transformation *T*<sub>θ</sub> to be applied to the feature map.

The localisation network function `can take any form`, such as a fully-connected network or a convolutional network, but should
include a `final regression layer` to produce the transformation parameters θ.

### 3.2 Parameterised Sampling Grid

To perform a warping of the input feature map, each output pixel is computed by applying a sampling kernel centered at a
particular location in the input feature map.

In general, the output pixels are defined to lie on a regular gird *G* = { *G*<sub>*i*</sub> } of pixels
*G*<sub>*i*</sub> = (*x*<sub>i</sub><sup>t</sup>, *y*<sub>i</sub><sup>t</sup>), forming an output feature map *V*.

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/correspondence_2_3.png" width=700>

For clarity of exposition, assume for the moment that *T*<sub>θ</sub> is a 2D affine transformation A<sub>θ</sub>.
In the affine case, the pointwise transformation is

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/correspondence_2_4.png" width=700>

The transformation allows cropping, translation, rotation, scale, and skew to be applied to the input feature map.

Also, transformation can also be `more general`, such as a plane projective transformation with 8 parameters,
piece wise affine, or a thin plate spline.

### 3.3 Differentiable Image Sampling




