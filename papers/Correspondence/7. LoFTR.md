
# LoFTR : Detector-Free Local Feature Matching with Transformers

[paper link here](https://arxiv.org/pdf/2104.00680.pdf)

## Abstract

We present a novel method for local image feature matching.
Instead of performing `image feature dectection, description, and matching sequentially`, we propose to first
establish `pixel-wise dense matches` at a coarse level and `later refine` the good matches at a fine level.

We use `self and cross attention layers in Transformer` to obtain feature descriptors that are conditioned on both images.

> The global receptive fields provided by Transforemr enables our method to produce dense matches in `low-texture areas`,
> where feature detectors usually struggle to preduce `repeatable interest points.`

## Introduction

Given two images to be matched, most existing matching methods consist of three seperate phase:
* feature detection
* feature description
* feature matching

In the detection phase, salient points like corners are first detected as interest points from each image.
Local desriptors are then extracted around neighborhod regions of these interest points.

The use of a feature detector reduces the search space of matching, and the resulted sparse correspondences are sufficient
for most tasks.

However, a feature detector may `fail to extract enough interset points` that are repeatable between images due to
various factors `such as poor texture, repetitive patterns, viewpoint change, illumination variation, and motion blur.`

Several recent works have attempted to remedy this problem by establishing pixel-wise dense matches.

However, the dense features extracted by CNN in these works have `limited receptive field` which `may not distinguish indistinctive regions.`

Instead, humans find correspondences in these indistinctive regions not only based on the local neighborhood,
but with a `larger global context.`

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/LoFTR_1.png" width=400>

**Motivated by the above observatinos, we propose Local Feature TRansformer (LoFTR), a novel detector-free apporach to local feature matching.**

The global receptive field and positional encoding of Transformer enble the transformed feature representations to be context-and
position-dependent.

## Methods

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/LoFTR_2.png" width=800>

Given the image pair *I*<sup>A</sup> and *I*<supp>B</sup>, we propose detector-free design, named LoFTR.

### 3.1 Local Feature Extraction

We use a standard CNN to extract multi-level features both images. We use *F*<sub>~</sub><sup>A</sup> and *F*<sub>~</sub><sup>B</sup> to denote
the coarse level features at 1/8 of the orignial dimension, and *F*<sub>^</sub><sup>A</sup> and *F*<sub>^</sub><sup>B</sup>
the fine-level features at 1/2 of the original image dimension.

### 3.2 Local Feature Transformer (LoFTR) Module

After the local feature extraction, *F*<sub>~</sub><sup>A</sup> and *F*<sub>~</sub><sup>B</sup> are passed through the LoFTR
module to extract position and context dependent local features. Intuitively, the LoFTR module transforms the features into
`feature representations that are easy to match.`

### 3.3 Establishing Coarse-level matches

Two types of differentiable matching layers can be applied in LoFTR.
* with on optimal transport layer
* dual-softmax operator

The socore matrix *S* between the transformed featuers is first caculated by
> *S*(*i*, *j*) = <*F*<sub>*tr*</sub><sup>A</sup>, *F*<sub>*tr*</sub><sup>B</sup>>

Then, we can apply softmax on both dimensions of *S* to obtain the probability of soft mutual
nearest neighbor matching. Formally, when using dual-softmax, the matching probability *P*<sub>c</sub> is obtained by:

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/LoFTR_3.png" width=500>

**Match Selection** : Based on the confidence matrix *P*<sub>c</sub>, we select matches with `confidence higher than
a thresold` of θ<sub>c</sub>, and further `enforce the mutual nearest neighbor criteria`, which filters possible outlier
coarse matches. We denote the coarse-level match predictions as:

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/LoFTR_4.png" width=500>

### 3.4 Coarse-to-Fine Module

After establishing coarse matches, these matches are refined to the original image resolution with the coarse-to-fine
module.

For every coarse match (*i*<sub>~</sub>, *j*<sub>~</sub>), we first locate its position (*i*<sup>^</sup>, *j*<sup>^</sup>)
at fine-level feature maps *F*<sup>A</sup> and *F*<sup>B</sup>, and then crop two sets of local windows of size *w* * *w*.

A smaller LoFTR module than transforms the cropped features within each window by *N*<sub>f</sub> times, two transformed local
feature maps *F*<sub>tr</sub><sup>^A</sup> and *F*<sub>tr</sub><sup>^B</sup> centered at *i*<sup>^</sup> and *j*<sup>^</sup>, respectively.

### 3.5 Supervision

The final loss consists of the losses for the coarse-level and the fine-level:

> *L* = *L*<sub>c</sub> + *L*<sub>f</sub>

#### Coarse-level supervision

The loss function for the coarse-level is the negative log-likelihood loss over the confidence matrix *P*<sub>c</sub> returned
by either the optimal transport layer or the dual-softmax operator.

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/LoFTR_5.png" width=400>

#### Fine-level supervision

We use the *l*<sub>2</sub> loss for fine-level refinement. 

For each query point *i*<sup>^</sup>, we also measure its uncertainty by calculating the total variance
σ<sup>2</sup>(*i*<sup>^</sup>) of the corresponding heatmap. The target is to optimize
the refined position that has low certainty, resulting in the final weighted loss function:

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/LoFTR_6.png" width=500>

