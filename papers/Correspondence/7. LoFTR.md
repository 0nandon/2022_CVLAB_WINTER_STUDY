
# LoFTR : Detector-Free Local Feature Matching with Transformers

[paper link here](https://arxiv.org/pdf/2104.00680.pdf)

## Abstract

We present a novel method for local image feature matching.
Instead of performing `image feature dectection, description, and matching sequentially`, we propose to first
establish `pixel-wise dense matches` at a coarse level and `later refine` the good matches at a fine level.

We use `self and cross attention layers in Transformer` to obtain feature descriptors that are conditioned on both images.

> The global receptive fields provided by Transforemr enables our method to produce dense matches in `low-texture areas`,
> where feature detectors usually struggle to preduce `repeatable interest points.`

## Introduction

Given two images to be matched, most existing matching methods consist of three seperate phase:
* feature detection
* feature description
* feature matching

In the detection phase, salient points like corners are first detected as interest points from each image.
Local desriptors are then extracted around neighborhod regions of these interest points.

The use of a feature detector reduces the search space of matching, and the resulted sparse correspondences are sufficient
for most tasks.

However, a feature detector may `fail to extract enough interset points` that are repeatable between images due to
various factors `such as poor texture, repetitive patterns, viewpoint change, illumination variation, and motion blur.`

Several recent works have attempted to remedy this problem by establishing pixel-wise dense matches.

However, the dense features extracted by CNN in these works have `limited receptive field` which `may not distinguish indistinctive regions.`

Instead, humans find correspondences in these indistinctive regions not only based on the local neighborhood,
but with a `larger global context.`

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/LoFTR_1.png" width=400>

**Motivated by the above observatinos, we propose Local Feature TRansformer (LoFTR), a novel detector-free apporach to local feature matching.**

The global receptive field and positional encoding of Transformer enble the transformed feature representations to be context-and
position-dependent.

## Methods

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/LoFTR_2.png" width=800>

Given the image pair *I*<sup>A</sup> and *I*<supp>B</sup>, we propose detector-free design, named LoFTR.

### 3.1 Local Feature Extraction

We use a standard CNN to extract multi-level features both images. We use *F*<sub>~</sub><sup>A</sup> and *F*<sub>~</sub><sup>B</sup> to denote
the coarse level features at 1/8 of the orignial dimension, and *F*<sub>^</sub><sup>A</sup> and *F*<sub>^</sub><sup>B</sup>
the fine-level features at 1/2 of the original image dimension.

### 3.2 Local Feature Transformer (LoFTR) Module

After the local feature extraction, *F*<sub>~</sub><sup>A</sup> and *F*<sub>~</sub><sup>B</sup> are passed through the LoFTR
module to extract position and context dependent local features. Intuitively, the LoFTR module transforms the features into
`feature representations that are easy to match.`

### 3.3 Establishing Coarse-level matches

Two types of differentiable matching layers can be applied in LoFTR.
* with on optimal transport layer
* dual-softmax operator

The socore matrix *S* between the transformed featuers is first caculated by
> *S*(*i*, *j*) = <*F*<sub>*tr*</sub><sup>A</sup>, *F*<sub>*tr*</sub><sup>B</sup>>

Then, we can apply softmax on both dimensions of *S* to obtain the probability of soft mutual
nearest neighbor matching. Formally, when using dual-softmax, the matching probability *P*<sub>c</sub> is obtained by:

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/LoFTR_3.png" width=500>

**Match Selection** : Based on the confidence matrix *P*<sub>c</sub>, we select matches with `confidence higher than
a thresold` of Î¸<sub>c</sub>, and further `enforce the mutual nearest neighbor criteria`, which filters possible outlier
coarse matches. We denote the coarse-level match predictions as:

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/LoFTR_4.png" width=500>
