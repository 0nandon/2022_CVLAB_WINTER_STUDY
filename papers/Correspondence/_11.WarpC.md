
# Warp Consistency for Unsupervised Learning of Dense Correspondences

[paper link here](https://arxiv.org/pdf/2104.03308.pdf)

## Abstract

The key challenge in learning dense correspondences lies in the `lack of ground-truth matches` for real image pairs.

We propose WarpC, an unsupervised learning objective for dense correspondence regression.
Our objective is effective even in settings with large appearance and view-point changes.

From our observations and empiricalresults, we disign a `general unsupervised objective` employing two of the derived constraints.

## Introduction

While supervised deep learning methods have achieved impressive resutls, they are limited by the availability of ground truth annotations.
In fact, collecting denes ground-truth correspondence data of real scenes is estremely challenging and costly, if not possible.

We introduce Warp Consistency, an `unsupervised learning objective for dense correspondence regression.` Unlike previous approaches, it is
capable of handling large appearance and view-point changes, while also generalizing to unseen real data.

From a real image pair (*I*, *J*), we construct a third image *I*<sup>'</sup> by warping *I* with a known flow field *W*, that is
created by `randomly sampling homographies`, from a specified distribution. We then consider the `consistency graph` arising from
the resulting image triplet (*I*, *I*<sup>'</sup>, *J*). By carefully anlyzing their properties, we propose an unsupervised loss
based on predicting the flow W by the composiiton *I*sup>'</sup> > *J* > *I* via image *J*.

We perform comprehensibe empirical analysis of the objectives derived from our warp consistency grapn and compare them to existing
unsupervised alternatives.

## Method

### 3.1 Problem formulation and notation

We denote flow field to *F*<sub>*I > J*</sub> ∈ R<sup>h * w * 2</sup>, and it is directly related to the mapping *M*<sub>*I > J*</sub>
∈ R<sup>h * w * 2</sup>, which encodes the absolute location in *J* corresponding to the pixel location in image *I*.

It is thus related to the flow through *M*<sub>*I > J*</sub>(*x*) = *x* + *F*<sub>*I > J*</sub>.

Also, We define the warping ø<sub>*F*</sub>(*T*) of a functino *T* : R<sup>2</sup> > R<sup>d</sup> by the flow
*F* as ø<sub>*F*</sub>(*T*)(*x*) = *T*(*x* + *F*(*x*)). This is more compactly expressed as ø<sub>*F*</sub>(*T*) = *T* • *M*<sub>*F*</sub>,
where • denotes function composition.

The goal of this work is to learn a neural network *f*<sub>θ</sub>, with parameters θ, that predicts an estimated flow *F*<sup>^</sup><sub>*I > J*</sub>
= *f*<sub>θ</sub>(*I, J*) relating *I* to *J*.

> We will consistently use the hat ^ to denote an estimated or predicted quantity.

### 3.2 Unsupervised data losses

<img src="" width="">

To develop our approach, we first briefly review relavant existing alternatives for unsupervised learning of flow.

> We call a learning formulation 'unsupervised' if it does `not require any information` other than pairs of images (*I, J*) depicting
> the same scene of object. Specifically, unsupervised methods do not require `any annotations made by the humans` or other matching algortithms.

* Photometric losses
* Forward-backward consistency : It is enforced by the trivial degenerate solution of always `predicting zero flow` *F*<sup>^</sup><sub>I > J</sub> = 
*F*<sup>^</sup><sub>J > I</sub> = 0
* Warp-supervision

### 3.3 Warp consistency graph

We set out to find a new unsupervised objective suitable for scenarios with `large appearance and view-point changes` where
photometric based lossees struggle.

To address these issues, we consider all possible consistency relations obtained from the three images involved in both aforementioned
objectives.

<img src="" width="">

There are three constraints we can think:
* Pair-wise constraints : they offer no advantage over standard warp-supervision.
* Bipath constraints
* Cycle constraints

Compared to the bipath constraints, the cycle variants require two consecutive warping operations, stemming from the additional
mapping composition. Each warp `reduces the valid region` and `introduces interpolation noise and artifacts in parctice.`

Constraints `involving fewer warping operations are thus desirable`, which is an advantage of the class of `bipath constraints.`

### 3.4 Bipath constraints

There are three exist three different bipath constraints that preserve the direction of the known warp *W*.

#### *I'J* - bipath

This constraint has a degenerate trivial solution. In fact, (4a) is satisfied for any *W* by simply mapping all inputs *x* to a 
constant pixel location *c* ∈ R<sup>2</sup>.

In order to satisfy this constant, the network can thus learn to predict `the same flow *F*<sup>^</sup> = c - I for any input image pair.`

#### *JI* - bipath

In this bipath, the resulting unsupervised loss is formulated as:

<img src="" width="">

Due to the `cancellation effect` between the estimated flow terms *F*<sup>^</sup><sub>*J > I'*</sub> and *F*<sup>^</sup><sub>*J > I*</sub>,
the objective (6) is insensitive to a constant bias in the prediction.

#### *W* - bipath

It leads to the *W* - bipath consistency loss,

<img src="" width="">

We first analye the limiting case ||*W*|| ≈ 0 by setting *W* = 0, which leads to standard forward-backward consistency, since *I*' = *I*.

The *W* - bipath is thus a `direct generalization` of the  latter constraint.

Importantly, by randomly sampling non-zero warp *W*, `degenrate solutions are avoided`, effectively solving the one fatal issue of forward-backward
consistency objectives.

Furthermore, compard to warp-supervision, it enables to directly learn the flow prediction *F*<sup>^</sup><sub>*J > I*</sub> between the real
pair (*I, J*).

### 3.5 Warp consistency loss

### 3.6 Sampling warps W

During training, we `randomly sample` it from a distribution *W* ~ *p*w, which we need to design.

As discussed in previous section, the W-bipath loss approaches the forward-backward consistency loss when `the magnitude
of the warps decreases` ||*W*|| > 0. Exclusively sampling too small warps *W* ≈ 0 therefore risks `biasing the prediction towards zero.`

On the other hand, too large warps would render estimation of *F*<sup>^</sup><sub>I' > J</sub> challenging and introduce unnecessary
invalid image regions.

As as rough guide, the distribution *pw* should yield warps of `similar magnitued as the real transformations` ||*F*<sub>*J > I*</sub>||,
thus giving similar impact to all three terms in our loss function.

We construct W by:
* sampling homography
* TPS
* affine-TPS transformations

To make the warps *W* harder, we optionally also `compose the flow obtained from three methods above.` with randomly sampled
elastic transforms.

## Couclusion

We propose an unsupervised learning objective for dense correspondences, paritculary suitable for scenarios with `large changes
in appearance and geometry.` When integrated into three recent dense correspondence networks, our approach outperforms SOTA for
`multiple geometric and semantic mathcing datasets.`












