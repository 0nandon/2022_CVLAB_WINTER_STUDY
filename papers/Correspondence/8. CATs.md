
# CATs: Cost Aggregation Transformers for Visual Correspondence

[paper link here](https://arxiv.org/pdf/2106.02520.pdf)

## Abstract

We propose a novel cost aggregation network, called Cost Aggregation Transformers(CATs), to find dense correspondences
between `semantically similar images` with additional challenges posed by large-intra class apperance and
geometric variations.

Specifically, we include a`ppearance affinity modeling` to aid the cost aggregation process in order to `disambiguate the noisy
initial correlation maps` and `propose multi-level aggregation` to efficiently capture different semantics from hierarchical feature
representations.

We then combine with swapping self-attention technique and residual connections not only to `enforce consistent matching`,
but also ease the learning process.

## Introduction

Unlike classical dense correspondence problems that consider visually similar images taken under the
geometrically constrained settings, semantic correspondence poses additional challenges from `large
intra-class appearacne` and `geometric variations` caused by the unconstrained settings of given image pair.

Recent approaches addressed this challenges by classical matching pipeline:
* feature extraction
* cost aggregation
* flow estimation

In this work, we focus on the `cost aggregation stage`, and propose a novel cost aggregation network to tackle
aforementioned issues.

* We concatenate an appearance embedding with the correlation map, which helps to disambiguate the correlation map within the Transformer.
* We use a `stack of correlation maps constructed from multi-level features`, and propose to effectively aggregate the scores
across the multi-level correlation maps.
* we consider `bidirectional` nature of correlation map, and leverage the correlation map from both directions, `obtaining
reciprocal scores` by swapping the pair of dimensions of correlation map in order to allow glpbal consensus in both perspective.
* We provide residual connections around aggregation networks in order to ease the learning process.

## Methodology

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/CATs_1.png" width=700>

## 3.1 Motivation and Overview

Estimating the correspondence with sole reliance on matching similarities between *D*<sub>s</sub> and *D*<sub>t</sub> is often
challenged by the `ambiguous matches` due to the `repetitive patterns` or `background clutters.`

To overcome these, we present Transformer-based cost aggregation networks that effectively integrate information present in all
pairwise matching costs.

### 3.2 Feature Extraction and Cost Computation

To extract dense feature maps from images, we follow that use `multi-level features` for construction of correlation maps.

Given a sequence of feature maps, we resize all the selected feature maps to R<sup>h * w * c</sup>, with height *h*, width *w*,
and *c* channels. The resized features then undergo *l*-2 normalization.

Given resized dense featrues *D*<sub>s</sub> and *D*<sub>t</sub>, we compute a cerrelation map *C* R<sup>*hw* * *hw*</sup> using
the inner product between features : *C*(*i*, *j*) = *D*<sub>s</sub>(*i*) * *D*<sub>t</sub>(*j*) with points *i* and *j* in the
target and source features, respectively. **In this way, all pairwise feature matches are computed and stored.**

However, raw matching scores contain numerous ambiguous matching points as which results inaccurate correspondences. To remedy this,
we propose cost aggregation networks in the following that aim to `refine the ambiguous or noisy matching scores.`

### 3.3 Transformer Aggregator

In this paper, we leverage the Transformers to integrate the matching scores to discover global consensus by considering global
context information. Specifically, we obtain a refined cost *C*' by feeding the raw cost *C* to the Transformer *T*, consisting
of self-attention, LN, and MLP modules:

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/CATs_2.png" width=400>
