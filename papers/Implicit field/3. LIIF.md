# Learning Continuous Image Representation with Local Implicit Image Function

## Abstract

In this papaer, we seek to learn a continuous representation for images.

Inspired by the recent progress in 3D reconstruction with implicit neural representation, we prepose LIIF,
which takes an `image cooridnate and the 2D deep features` around the coordinate as inputs, predict the `RGB value` at a given
coordinate as an output.

Since the coordinates are continuous, LIIF can be presented in arbitrary resolution.

## Introduction

While the pixel-based representation has been succesfully applied in various computer vision tasks, they are also constrained
by the resolution.

If we want to train a CNN, we will usually need to resize the images to the same size, which may sacrifice fidelity.
Instead of representing an image with a `fixed resolution`, we propose to study a `continuous representation for images.`
By modeling an image as a function defined in a continuous domain, we can restore and generate the image in `arbitrary resolution`
if needed.

In this paper, we propose the LIIF for representing natural and complex images in a continuous manner.
In LIIF, an image is represented as a `set of latent codes distributed in spatial dimensions.`

We train an encoder with the LIIF representation via a `self-supervised task with super-resolution`, where the
input and ground-truth are provided in continuously changing up sampling scales.

We further demonstrate that LIIF builds a bridge between discrete and continuous representation in 2D.

Our contributions include:
* A novel method for representing natural and complex images continuously
* LIIF representation allows extrapolation to even `30 higher resolution` which is not presented during training time.
* We show LIIF representation is effective for the learning tasks with `size-varied image ground-truths.`

### Local Implicit Image Function

In LIIf representation, each continuous image *I*<sup>(i)</sup> is represented as a 2D feature map *M*<sup>(i)</sup> ∈ R<sup>H * W * D</sup>.
A decoding function *f*<sup>θ</sup> is shared by all the images, it is parameterized as a MLP and takes the form:

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/LIIF_1.png" width=400>

We assume the *H* * *W* featuer vectors of *M*<sup>(i)</sup> are evenly `distributed in the 2D space of the continuous image` domain
of *I*<sup>(i)</sup>, then we assign a `2D coordinate to each of them.` For the continuous image *I*<sup>(i)</sup>, the RGB value at
coordinate *x*<sub>q</sub> is defined as:

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/LIIF_2.png" width="400">

Each latent code *z* in *M*<sup>(i)</sup> represents a local piece of the continuous image.

#### Feature unfolding

`To enrich the information` contained in each latent code in *M*<sup>(i)</sup>, we apply feature unfolding to *M*<sup>(i)</sup> and
*M*<sup>^(i)</sup>. A latent code in is the concatenation of the 3 * 3 neighboring latent codes in *M*<sup>(i)</sup>.
Formally, the feature unfolding is defined as:

<img src="https://github.com/0nandon/2022_CVLAB_WINTER_STUDY/blob/main/photo/LIIF_3.png" width="500">

#### Local ensemble

An issue in Eq (2) is the `discontinuous prediction.` Specifically, since the signal prediction at *x*<sub>q</sub> is done by querying the nearest
latent code *z*<sup>*</sup> in *M*<sup>(i)</sup>, when *x*<sub>q</sub> moves in the 2D domain, the selection of *z*<sup>*</sup> `can suddenly
switch from one to another.`

<img src="" width="">


Intuitively, this is to let local pieces represented by local latent codes `overlap with its neighboring pieces` so that at each coordinate there
are four latent codes for `independently predicting the signal.`

It achieves continuous transition at coordinates where *z*<sup>*</sup> switches.

#### Cell decoding

We use cell decoding, which allow the implicit functino to predict `different values for differently
sized pixels at the same location.`

<img src="" width="">

As we will show in the experiments, having an extra input *c* can be beneficial when presenting the continuous representation in
a given resolution.


### Learning Continuous Image Representation

In this section, we introduce the method for learning to generate a continuous representatino for an image, an overview is
demonstrated in Figure 4.

<img src="" width="">

The general idea is to train an Encoder *E*<sup>ø</sup> that maps an image to a 2D feature map as its LIIF representaion, 
the function *f*<sub>θ</sub> shared by all the images is jointly trained.

For maintaining `high fidelity` even when being presented in higher resolution, we prepose to train the framework in a `self-supervised
task with super resolution.`

* *x*<sub>hr</sub> : center coordinates of pixels in the image domain
* *s*<sub>hr</sub> : corresponding RGB values of the pixels.

We use L1 loss in our experiment.

## Conclusion

In this paper, we presented the LIIF for continuous image representation.

By training an encoder with LIIF representaion in a self supervised task with super-resolution, it cah generate continuous
LIIF representation for pixel-based images. The continuous representation can be presented in extreme high resolution,
we showed that it can generalize to much `higher precision than the training scales` while maintaining high fidelity.
